{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20180731-1f4f-4d09-9130-91617504aaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This script will be used to perform the data cleaning and preparation tasks required for unsupervised learning\n",
    "\n",
    "#import necessary packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Import relavent pyscripts:\n",
    "import EDA_part2  # I'd suggest importing specific functions from EDA_part2.\n",
    "                  # from EDA_part2 import foo\n",
    "                  # I don't know what is in EDA_part2 and it could have extra functions that would just slow the program down\n",
    "\n",
    "# Added type hints to all the functions, super easy way to hint\n",
    "def transform_data(input_df: pd.DataFrame, remove_cols: list[str], cat_cols: list[str], y_col: list[str], stop_after_data_split: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function prepares the dataframe for model training by cleaning the dataframe, standardizing quantitative inputs, and one-\n",
    "    Hot encoding the categorical data\n",
    "\n",
    "    INPUTS:\n",
    "    input_df: a pandas dataframe object of the combined charging data\n",
    "    remove_cols: a list of columns to remove from the dataset (not relavent to our study)\n",
    "    cat_cols: a list of string entries detailing which columns are categorical and should be one-hot encoded\n",
    "    y_col: a string entry title for the column that we're performing unsupervised learning on\n",
    "\n",
    "    OUTPUTS:\n",
    "    output_df: a pandas dataframe object that is scaled and one-hot encoded with the desired features\n",
    "    \"\"\"\n",
    "\n",
    "    # First remove columns unwanted:\n",
    "    temp_df = input_df.drop(columns=remove_cols)\n",
    "\n",
    "    # Make subset df that does not include the target column(s)\n",
    "    if isinstance(y_col, list):\n",
    "        features_df = temp_df.drop(columns=y_col)\n",
    "        target_df = temp_df[y_col]\n",
    "    else:\n",
    "        features_df = temp_df.drop(columns=[y_col])\n",
    "        target_df = temp_df[[y_col]]\n",
    "    # This might be better as a try/except if you're worried about generating a fail\n",
    "    # If you just don't want a series I'd suggest converting `y_col` to a list\n",
    "    # def force_variable_to_list(x: list | str) -> list:\n",
    "    #     return x if isinstance(x, list) else [x]\n",
    "\n",
    "    # Separate test data from train/val set:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_df,target_df, test_size=0.2, random_state=42)\n",
    "\n",
    "    if stop_after_data_split:\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    # Find which columns are quantitative by deduction\n",
    "    all_cols = list(features_df.columns)\n",
    "    quant_cols = [col for col in all_cols if col not in cat_cols]  # I've been using num_col or num_features to be consistent with\n",
    "                                                                   # the industry standards\n",
    "\n",
    "    # Set the encoder & scaler\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop=None)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit and transform the columns appropriately\n",
    "    X_train_scaled_columns = scaler.fit_transform(X_train[quant_cols])\n",
    "    X_train_encoded_columns = encoder.fit_transform(X_train[cat_cols])\n",
    "    X_test_scaled_columns = scaler.transform(X_test[quant_cols])\n",
    "    X_test_encoded_columns = encoder.transform(X_test[cat_cols])\n",
    "\n",
    "    # Now map back to a scaled, encoded dataframe:\n",
    "    X_train_scaled_df = pd.DataFrame(X_train_scaled_columns, columns=quant_cols)\n",
    "    X_train_encoded_df = pd.DataFrame(X_train_encoded_columns, columns=encoder.get_feature_names_out(cat_cols))\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled_columns, columns=quant_cols)\n",
    "    X_test_encoded_df = pd.DataFrame(X_test_encoded_columns, columns=encoder.get_feature_names_out(cat_cols))\n",
    "\n",
    "    # Combine the processed data\n",
    "    X_train_scale_encoded_df = pd.concat([X_train_encoded_df, X_train_scaled_df], axis=1)\n",
    "    X_test_scale_encoded_df = pd.concat([X_test_encoded_df, X_test_scaled_df], axis=1)\n",
    "\n",
    "    return(X_train_scale_encoded_df, X_test_scale_encoded_df, y_train, y_test)\n",
    "\n",
    "\n",
    "def process_datetime(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes an input_dataframe for the charging events and parses the datetime column to extract the characteristic information from it\n",
    "    such as day, month, year, day of the week, day of the year, hour, etc.\n",
    "\n",
    "    INPUTS:\n",
    "    input_df: a pandas dataframe object\n",
    "\n",
    "    OUTPUS:\n",
    "    out_df: a pandas dataframe with added columns for the specific date information\n",
    "    \"\"\"\n",
    "\n",
    "    # Given the start_datetime, extract key features\n",
    "    input_df[\"start_datetime\"] = pd.to_datetime(input_df[\"start_datetime\"])\n",
    "    input_df[\"year\"] = input_df[\"start_datetime\"].dt.year.astype(int)\n",
    "    input_df[\"month\"] = input_df[\"start_datetime\"].dt.month.astype(int)\n",
    "    input_df[\"day\"] = input_df[\"start_datetime\"].dt.day.astype(int)\n",
    "    input_df[\"hour\"] = input_df[\"start_datetime\"].dt.hour.astype(int)\n",
    "    input_df[\"minute\"] = input_df[\"start_datetime\"].dt.minute.astype(int)\n",
    "    input_df[\"second\"] = input_df[\"start_datetime\"].dt.second.astype(int)\n",
    "    input_df[\"time_of_day\"] = input_df[\"hour\"] + input_df[\"minute\"]/60 + input_df[\"second\"]/3600\n",
    "    input_df[\"weekday\"] = input_df[\"start_datetime\"].dt.weekday.astype(int)\n",
    "    input_df[\"day_of_year\"] = input_df[\"start_datetime\"].dt.dayofyear.astype(int)\n",
    "\n",
    "    # Now convert each of these to numeric rather than datetime:\n",
    "    # Is there something here?\n",
    "\n",
    "    # Why not just return input_df or change input_df to df and return that?\n",
    "    out_df = input_df\n",
    "\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def anomaly_tags(input_df: pd.DataFrame, anomaly_list: list[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts entries from the input_df's \"flag_id\" column to 0 or 1 based on anomaly prescence\n",
    "    INPUTS:\n",
    "    input_df: a pandas dataframe with the flag_id column that needs to be updated\n",
    "    anomaly_list: a list of integer values associated with the flag_id entry code\n",
    "\n",
    "    OUTPUTS:\n",
    "    out_df: a pandas dataframe with the updated flag_id column\n",
    "    \"\"\"\n",
    "\n",
    "    ########### What is this doing?\n",
    "    if anomaly_list == None:\n",
    "        input_df[\"flag_id\"] = input_df[\"flag_id\"].map(lambda x: 1 if x != 0 else 0)\n",
    "    else:\n",
    "        input_df[\"flag_id\"] = input_df[\"flag_id\"].map(lambda x: 1 if x in anomaly_list else 0)\n",
    "\n",
    "    # Same thing: why rename it?\n",
    "    out_df = input_df\n",
    "\n",
    "    return out_df\n",
    "\n",
    "\n",
    "def main_execution(input_condition: int = 2, test_ratio: float = 0.2, anomaly_list: list[int] = None) -> np.array:\n",
    "    \"\"\"\n",
    "    This function runs the main execution to process the data into train, val, and test datasets\n",
    "    INPUTS:\n",
    "    input_condition: An integer value (0, 1, or 2) specifying which condition to apply to combine the data\n",
    "                     0: do nothing, 1: drop nulls, 2: drops impractical values (used for supervised learning)\n",
    "    test_ratio: a float value between 0 and 1 determining how much of the input data will be the test dataset\n",
    "    anomaly_list: a list of integer values associated with the flag_id entry code\n",
    "\n",
    "    OUTPUTS:\n",
    "    X_train, X_val, & X_test:\n",
    "    Y_train, y_val, & y_test:\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the dataframe processed\n",
    "    merged_df = EDA_part2.combine_charging_data(input_condition=input_condition)\n",
    "    merged_time_df = process_datetime(merged_df)\n",
    "    mapped_df = anomaly_tags(merged_time_df, anomaly_list=anomaly_list)\n",
    "    mapped_df = mapped_df.dropna()\n",
    "\n",
    "    # Declare the target, categorical, and unwanted columns:\n",
    "    y_col = \"flag_id\"\n",
    "    cat_cols = [\"power_kw\", \"connector_type\", \"pricing\", \"region\", \"land_use\", \"metro_area\", \"charge_level\", \"venue\"]\n",
    "    remove_cols = [\"session_id\", \"connector_id_x\", \"evse_id\", \"connector_id_y\", \"start_datetime\", \"end_datetime\",\n",
    "                   \"hour\", \"minute\", \"second\"]\n",
    "\n",
    "    # Get the train, val, and test split of data\n",
    "    # EDITED HERE @SZYMON\n",
    "    X_train, X_test, y_train, y_test = transform_data(mapped_df, remove_cols=remove_cols, cat_cols=cat_cols, y_col=y_col, stop_after_data_split=True)\n",
    "\n",
    "    # Output to datafile for future calling if not already in cwd:\n",
    "    dir_contents = os.listdir()\n",
    "    outdict = {\"UL_Xtrain.csv\": X_train, \"UL_Xtest.csv\": X_test, \"UL_ytrain.csv\": y_train, \"UL_ytest.csv\": y_test}\n",
    "    for k,v in outdict.items():\n",
    "        if k not in dir_contents:\n",
    "            v.to_csv(k, index=False)  # Neat way to save this!\n",
    "\n",
    "    return X_train, X_test, y_train, y_test  # No need for the parenthesis, python already returns as a tuple\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_train, X_test, y_train, y_test = main_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8362a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
